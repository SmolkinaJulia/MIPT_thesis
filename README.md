# MIPT_thesis
# Магистерская диссертация
## Содержание репозитория
Thesis and science-related, research work during my masters at MIPT

В реопзитории сожержутся файлы-скрипты с экспериментами и некоторыми результатами, ответами на вопросы Гос эказамена, текст диссертации (будет добавлен позже) и другое.

## Основная информация
|ФИО обучающегося	            |Смолкина Юлия Александровна| -
| ------------- |:------------------:| -----:|
|Физтех-школа/факультет, группа|ФПМИ, ТИИ М06-106    | - 
|Базовая организация, кафедра	  |Научно-образовательный центр когнитивного моделирования| - 
|Тема НИР	                    |Обучение эмбеддингов задач для нейросетевых моделей классификации|Возможны изменения или дополнения 
|Текущие результаты|----|Дополняется  
|Наборы данных|mnist, fashoin mnist, pascal voc 2007, imagine net, cifar10|Дополняется 


## Вспомогательные материалы
> Референсы (статьи, теоретические материалы, исследования):
> 
> https://towardsdatascience.com/the-utility-of-task-embeddings-e00a18133f77
> https://github.com/cvxgrp/pymde/blob/main/examples/fashion_mnist.ipynb
> https://github.com/botkop/mnist-embedding/blob/master/notebooks/mnist-embedding-classifier.ipynb
> https://pymde.org
> https://github.com/tensorpack/tensorpack/blob/master/examples/SimilarityLearning/embedding_data.py
> https://keras.io/api/datasets/
> https://medium.com/coinmonks/how-to-get-images-from-imagenet-with-python-in-google-colaboratory-aeef5c1c45e5
> https://pymde.org/datasets/index.html
> https://github.com/MatthewWilletts/Embeddings/blob/master/make_embeddings.py
> https://habr.com/ru/post/666314/
> https://habr.com/ru/company/leader-id/blog/529012/
> https://vk.com/@papersreaders-graph-rise-graph-regularized-image-semantic-embedding
> https://arxiv.org/pdf/1902.10814.pdf

## Формальная и неформальная постановка задачи и ттеоретические выкладки
Что такое эмбеддинги и как они помогают искусственному интеллекту понять мир людей

Термин «эмбеддинг» (от англ. embedding – вложение) - стал часто встречаться в описаниях систем искусственного интеллекта только в последние несколько лет, а впервые появился в работах специалистов по обработке текстов на естественных языках. Естественный язык – это привычный способ общения людей. Например, язык машин – это двоичный код, в который компилируются все другие языки программирования. Однако в нашем случае речь идет именно об обработке естественного языка человека.

В русскоязычной литературе эмбеддингами обычно называют именно такие числовые векторы, которые получены из слов или других языковых сущностей. Напомню, что числовым вектором размерности k называют список из k чисел, в котором порядок чисел строго определен. Например, трехмерным вектором можно считать (2.3, 1.0, 7.35), а (1, 0, 0, 2, 0.1, 0, 0, 7.9) – восьмимерным числовым вектором.

Будем обучать модель для генерации эмбеддингов на задаче классификации таким образом, чтобы эмбеддинги похожих изображений (соседних вершин в графе) были как можно ближе друг к другу, то есть будем штрафовать модель если она ставит эмбеддинги похожих объектов далеко друг от друга.
Более формально, при обучении минимизируется следующая функция потерь (как пример):

где L(theta) — cross-entropy loss, Omega(theta) — регуляризатор, w_{u,v} — вес ребра, и d(.,.) — функция расстояния между эмбеддингами.

![alt-текст](https://github.com/SmolkinaJulia/MIPT_thesis/blob/main/pictures/theory/7zcHKeylpL0.jpg "alpha - hyperparameter")

Для того чтобы из предсказанного Image Embedding’a получить вероятности классов, эмбеддинг пропускается через полносвязный слой с последующим вычислением softmax’a.
Во время обучения вместе с размеченной картинкой u также выбирается картинка v, которая является соседней в графе, после чего вычисляется loss R(theta).
Важно отметить, что так как граф похожести используется только на этапе обучения, то время инференса остается прежним.


## Проведенные эксперименты и Текущее состояние НИР
`Дописать`

### Используемые подходы и архитектуры

* FCNN
* Группа сетей VGG
* Группа сетей DenseNet
* Группа сетей ResNet
* Группа сетей ResNeXt
* Группа сетей ReXNet/ResNeSt/Res2Net
* Группа сетей RegNet
* Группа сетей Inception/Xception
* Группа сетей MNASNet/NASNet/PnasNet/SelecSLS/DLA/DPN
* Группа сетей MobileNet/MixNet/HardCoRe-NAS
* Группа сетей трансформеров BeiT/CaiT/DeiT/PiT/CoaT/LeViT/ConViT/Twins
* Группа сетей ViT (Visual Transofrmer)
* Группа сетей ConvNeXt
* Группа сетей ResMLP/MLP-Mixer
* Группа сетей NFNet-F
* Группа сетей EfficientNet
* Прочие предобученные модели сетей

### Метрики для оценивания качества результатов
`distance measures`

Мера расстояния обычно количественно определяет несходство двух векторов признаков. Мы вычисляем его как расстояние между двумя векторами в некотором метрическом пространстве.

* Manhattan distance,
* Mahalanobis distance, 
* Histogram Intersection Distance (HID)

`similarity metrics`

Метрика подобия количественно определяет сходство между двумя векторами признаков. Таким образом, это работает противоположно метрикам расстояния: наиболее значимое значение показывает изображение, похожее на изображение запроса.

Например, косинусное расстояние (cosine distance) измеряет угол между двумя векторами признаков.

## План развития работы
`1`

`2`

`3`

## Рекомендации по эксплуатации
`Дописать`
